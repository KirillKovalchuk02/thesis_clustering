{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5f93a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from functions import join_stocks_crypto, run_clustering_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d1551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kshape_results = pd.read_csv('new_balance_silhouette/kshape.csv')\n",
    "results = kshape_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6095e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "results[['silhouette_norm']] = MinMaxScaler().fit_transform(results[['silhouette_score']])\n",
    "results['delta_norm'] = 1 - MinMaxScaler().fit_transform(results[['min_max_delta']])\n",
    "results['total_score'] = (results['silhouette_norm'] + results['delta_norm']) / 2\n",
    "\n",
    "best_configs = results.loc[results.groupby('method')['total_score'].idxmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f62f3df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PRCT      0.692412\n",
       "TTD       0.708151\n",
       "3YU.F     0.747658\n",
       "MDB       0.755362\n",
       "SITM      0.765612\n",
       "RNA       0.797502\n",
       "AXSM      0.814172\n",
       "DJT       0.901152\n",
       "TBA.F     0.916075\n",
       "ECX       0.923325\n",
       "RYTM      0.941110\n",
       "OL5.F     1.315230\n",
       "SMMT      1.362448\n",
       "4AH1.F    1.370364\n",
       "2B5.F     1.385327\n",
       "PJXC.F    1.894676\n",
       "LPS1.F    3.012170\n",
       "WK0.F     3.041260\n",
       "0DH.F     4.547030\n",
       "LFU2.F    4.970240\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_all_stocks = pd.read_csv('stocks_data_filled.csv',index_col='Date')\n",
    "cryptos_df = pd.read_csv('cryptos_data_new.csv', index_col='timestamp')\n",
    "joined_df = join_stocks_crypto(cryptos_df, df_all_stocks, mode = 'stocks_left')\n",
    "\n",
    "cryptos_list = list(cryptos_df.columns)\n",
    "\n",
    "\n",
    "log_returns = np.log(df_all_stocks / df_all_stocks.shift(1)).dropna()\n",
    "daily_volatility = log_returns.std()\n",
    "annual_volatility = daily_volatility * np.sqrt(252)\n",
    "\n",
    "annual_volatility.sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49cd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stocks = df_all_stocks.drop(columns=['OL5.F', 'SMMT', '4AH1.F', '2B5.F', 'PJXC.F', 'LPS1.F', 'WK0.F', '0DH.F', 'LFU2.F'])\n",
    "\n",
    "df_all_stocks.to_csv('stocks_data_filtered_volatility.csv')\n",
    "\n",
    "tickers = list(df_all_stocks.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c26f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470e26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1081c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  170 of 170 completed\n"
     ]
    }
   ],
   "source": [
    "from functions import get_tickers_stocks, get_close_prices, double_listed_stocks\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#GET THE STOCKS\n",
    "\n",
    "# us_exchanges = ['NMS', 'NYQ', 'NGM']\n",
    "# eu_exchanges = ['PAR', 'FRA', 'LSE', 'AMS']\n",
    "# asia_exchanges = ['SHH', 'JPX', 'HKG']\n",
    "\n",
    "# selected_exchanges = us_exchanges + eu_exchanges + asia_exchanges\n",
    "\n",
    "# full_selected_stocks = {}\n",
    "# df_all_stocks = pd.DataFrame()\n",
    "# for exchange in selected_exchanges:\n",
    "#     print(f'Extracting from {exchange}')\n",
    "#     exchanges = [exchange]\n",
    "#     selected_stocks_dict, ticker_list = get_tickers_stocks(50000, exchanges, 20)\n",
    "\n",
    "#     full_selected_stocks.update(selected_stocks_dict)\n",
    "\n",
    "if len(tickers) > 0: \n",
    "    df = get_close_prices(tickers, period = 1, start = '2024-01-01')\n",
    "    #df_all_stocks = pd.concat([df_all_stocks, df], axis=1)\n",
    "\n",
    "# doubly_listed_tickers = double_listed_stocks(df)\n",
    "\n",
    "# for ticker_to_drop in doubly_listed_tickers:\n",
    "#     try:\n",
    "#         df_all_stocks = df_all_stocks.drop(columns=[ticker_to_drop])\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "df_all_stocks = df.ffill() #ffill again after concatenating the tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f165bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stocks.to_csv('stocks_data_out_sample_2024_foltered_volatility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa65cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(portfolio_dict:dict, returns_for_portfolio:pd.DataFrame, n_sims=100, t=100, distribution_model='multivar_norm', \n",
    "                   plot=False, initialPortfolio=100, winsorize=False, winsorize_limits=(0.01, 0.01)):\n",
    "    returns_for_portfolio = returns_for_portfolio[list(portfolio_dict.keys())]\n",
    "    \n",
    "    if winsorize:\n",
    "\n",
    "        winsorized_returns = returns_for_portfolio.copy()\n",
    "        \n",
    "        for col in winsorized_returns.columns:\n",
    "            winsorized_returns[col] = mstats.winsorize(winsorized_returns[col], limits=winsorize_limits)\n",
    "        \n",
    "\n",
    "        returns_for_portfolio = winsorized_returns\n",
    "\n",
    "\n",
    "    mean_returns = returns_for_portfolio.mean()\n",
    "    cov_matrix = returns_for_portfolio.cov()\n",
    "\n",
    "    weights = [v for _, v in portfolio_dict.items()]\n",
    "\n",
    "    meanM = np.tile(mean_returns, (t, 1))  # Shape: (T, n_assets)\n",
    "\n",
    "    portfolio_sims = np.zeros((t, n_sims))\n",
    "\n",
    "    L = np.linalg.cholesky(cov_matrix)  # Cholesky decomposition\n",
    "\n",
    "    for sim in range(n_sims):\n",
    "\n",
    "        if distribution_model == 'bootstrap':\n",
    "            sampled_returns = returns_for_portfolio.bfill().sample(n=t, replace=True).values\n",
    "            portfolio_returns = sampled_returns @ weights\n",
    "        elif distribution_model in ['multivar_norm', 'multivar_t']:\n",
    "\n",
    "            if distribution_model == 'multivar_norm':\n",
    "                Z = np.random.normal(size=(t, len(portfolio_dict)))  # Shape: (T, n_assets)\n",
    "                daily_returns = meanM + Z @ L.T  # Shape: (T, n_assets)\n",
    "            elif distribution_model == 'multivar_t':\n",
    "                df = 25  # degrees of freedom\n",
    "                Z = np.random.normal(size=(t, len(portfolio_dict)))\n",
    "                chi2 = np.random.chisquare(df, size=(t, 1))\n",
    "                Z_t = Z / np.sqrt(chi2 / df)  # now Z_t has t-distributed marginals\n",
    "\n",
    "                daily_returns = meanM + Z_t @ L.T\n",
    "\n",
    "            portfolio_returns = daily_returns @ weights  # Shape: (T,)\n",
    "        \n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        portfolio_sims[:, sim] = np.cumprod(1 + portfolio_returns) * initialPortfolio\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(portfolio_sims)\n",
    "        plt.title(\"Monte Carlo Simulated Portfolio Paths\")\n",
    "        plt.xlabel(\"Days\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.show()\n",
    "\n",
    "    return portfolio_sims\n",
    "\n",
    "def out_of_sample_analysis(portfolio, price_data, start_date, period=126, \n",
    "                           calculate_metrics=True, plot_results=False, risk_free_rate=0.02, use_log_returns=True):\n",
    "    \"\"\"\n",
    "    Perform traditional out-of-sample analysis for a portfolio over a specified forward period.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolio : dict\n",
    "        Dictionary with tickers as keys and weights as values\n",
    "    price_data : pandas.DataFrame\n",
    "        DataFrame with dates as index and tickers as columns containing PRICES (not returns)\n",
    "    start_date : str or datetime\n",
    "        Date marking the beginning of out-of-sample period\n",
    "    period : int\n",
    "        Number of days for the forward period to analyze (default 126 ~= 6 months of trading days)\n",
    "    calculate_metrics : bool\n",
    "        Whether to calculate and return performance metrics\n",
    "    plot_results : bool\n",
    "        Whether to generate and return performance plots\n",
    "    risk_free_rate : float\n",
    "        Annual risk-free rate used for Sharpe ratio calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing performance metrics and return series for the period\n",
    "    \"\"\"\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "    \n",
    "    results = {}\n",
    "    tickers = list(portfolio.keys())\n",
    "    weights = np.array([portfolio[ticker] for ticker in tickers])\n",
    "    \n",
    "    missing_tickers = [ticker for ticker in tickers if ticker not in price_data.columns]\n",
    "    if missing_tickers:\n",
    "        raise ValueError(f\"The following tickers are missing from price_data: {missing_tickers}\")\n",
    "    \n",
    "    price_subset = price_data[tickers].copy()\n",
    "    price_subset = price_subset.sort_index()\n",
    "    oos_data = price_subset[price_subset.index >= start_date]\n",
    "    \n",
    "    if oos_data.empty:\n",
    "        raise ValueError(f\"No data available after start_date: {start_date}\")\n",
    "    \n",
    "    end_date = start_date + timedelta(days=period)\n",
    "    \n",
    "    # Folter data for this period\n",
    "    period_mask = (oos_data.index <= end_date)\n",
    "    period_data = oos_data.loc[period_mask]\n",
    "    \n",
    "\n",
    "\n",
    "    returns = period_data.dropna()\n",
    "    original_returns = returns.copy()\n",
    "\n",
    "    if use_log_returns:\n",
    "        invalid_mask = returns <= -1.0\n",
    "        if invalid_mask.any().any():\n",
    "            invalid_stocks = returns.columns[invalid_mask.any()]\n",
    "\n",
    "            log_compatible_returns = returns.copy()\n",
    "\n",
    "            log_compatible_returns[invalid_mask] = -0.99\n",
    "            \n",
    "            log_returns = np.log(1 + log_compatible_returns)\n",
    "        else:\n",
    "            log_returns = np.log(1 + returns)\n",
    "            \n",
    "        portfolio_log_returns = log_returns.dot(weights)\n",
    "        \n",
    "        # Convert back to simple returns for statistics\n",
    "        portfolio_returns = np.exp(portfolio_log_returns) - 1\n",
    "        \n",
    "        # Calculate cumulative returns properly from log returns\n",
    "        cumulative_returns = np.exp(portfolio_log_returns.cumsum()) - 1\n",
    "    else:\n",
    "        # Simple returns method\n",
    "        portfolio_returns = returns.dot(weights)\n",
    "        cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = {\n",
    "        'returns': portfolio_returns,\n",
    "        'cumulative_returns': cumulative_returns\n",
    "    }\n",
    "    \n",
    "    if calculate_metrics:\n",
    "        total_return = cumulative_returns.iloc[-1] if len(cumulative_returns) > 0 else 0\n",
    "        trading_days = len(portfolio_returns)\n",
    "        \n",
    "        # Annualization factor\n",
    "        annual_factor = 252 / trading_days\n",
    "        annualized_return = (1 + total_return) ** annual_factor - 1\n",
    "        original_portfolio_returns = original_returns.dot(weights)\n",
    "        volatility_annual = original_portfolio_returns.std() * np.sqrt(252)\n",
    "        sharpe_ratio = (annualized_return - risk_free_rate) / volatility_annual if volatility_annual > 0 else 0\n",
    "        non_annual_sharpe = (total_return - risk_free_rate) / portfolio_returns.std()\n",
    "    \n",
    "        \n",
    "        if len(portfolio_returns) > 2:\n",
    "            var_95 = abs(portfolio_returns.quantile(0.05))\n",
    "        \n",
    "        results.update({\n",
    "            'mean_period_return': total_return,\n",
    "            'annualised_return': annualized_return,\n",
    "            'sharpe_annualized': sharpe_ratio,\n",
    "            'sharpe_period': non_annual_sharpe,\n",
    "            'var_95': var_95,\n",
    "        })\n",
    "    \n",
    "    if plot_results and len(cumulative_returns) > 0:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot cumulative returns\n",
    "        cumulative_returns.plot(ax=ax1, title=f'{period}-Day Cumulative Returns')\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax1.set_ylabel('Cumulative Return')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot drawdowns\n",
    "        drawdowns.plot(ax=ax2, title=f'{period}-Day Drawdowns', color='red')\n",
    "        ax2.set_ylabel('Drawdown')\n",
    "        ax2.set_ylim(bottom=min(drawdowns.min() * 1.1, -0.05), top=0.01)\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        results['plot'] = fig\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def simulate_evaluate_portfolio_subset(portfolios_subset:dict, return_df, n_sims=100, t=100, distribution_model='out_sample_direct', rf_annual=0.04, seed=30,\n",
    "                                       winsorize=False, winsorize_limits=(0.01, 0.01)):\n",
    "    simulations_results_dict = dict()\n",
    "    subset_statistics_df = pd.DataFrame()\n",
    "\n",
    "    random.seed(seed)\n",
    "    #if distribution_model=='out_sample_direct':\n",
    "            #print(\"Don't forget to put 2024 data in return_df!!!!\")\n",
    "\n",
    "    for i, portfolio_dict in portfolios_subset.items():\n",
    "        returns_portfolio = return_df[list(portfolio_dict.keys())]\n",
    "\n",
    "        #Making sure weights sum up to 1\n",
    "        total = sum(portfolio_dict.values())\n",
    "        portfolio_dict = {k: v / total for k, v in portfolio_dict.items()}\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if distribution_model=='out_sample_direct':\n",
    "            res = out_of_sample_analysis(portfolio_dict, return_df, '2024-01-01', period=t, calculate_metrics=True, risk_free_rate=rf_annual, plot_results=False, use_log_returns=False)\n",
    "            mean_annual_return = res['annualised_return']\n",
    "            mean_return = res['mean_period_return']\n",
    "            sharpe_annual = res['sharpe_annualized']\n",
    "            VaR_final = res['var_95']\n",
    "            sharpe_period = res['sharpe_period']\n",
    "\n",
    "            stat_results = pd.DataFrame({'annualised_return': [mean_annual_return],\n",
    "                                     'mean_period_return': [mean_return],\n",
    "                                     'sharpe_annualized': [sharpe_annual],\n",
    "                                     'sharpe_period': [sharpe_period],\n",
    "                                     'VaR': [VaR_final],\n",
    "                                     })\n",
    "            \n",
    "            subset_statistics_df = pd.concat([subset_statistics_df, stat_results])\n",
    "\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            portfolio_sims = run_simulation(portfolio_dict, returns_portfolio, n_sims=n_sims, t=t, distribution_model=distribution_model, plot=False, winsorize=winsorize, winsorize_limits=winsorize_limits)\n",
    "\n",
    "\n",
    "\n",
    "            simulations_results_dict[i] = portfolio_sims\n",
    "\n",
    "\n",
    "            #CALCULATE STATISTICS PER PORTFOLIO:\n",
    "            daily_returns = (portfolio_sims[1:, :] - portfolio_sims[:-1, :]) / portfolio_sims[:-1, :] #DO WE WANT TO ADD FIRST ROW OF 100 VALUES SO THAT WE HAVE A FULL THING?\n",
    "            #average cumulative return \n",
    "            cumulative_returns_per_simulation = calculate_cumulative_returns(daily_returns)\n",
    "            #mean_cumulative_return_for_portfolio = np.mean(cumulative_returns_per_simulation)\n",
    "            #average daily return\n",
    "            mean_daily_return_for_portfolio = np.mean(daily_returns)\n",
    "            #STD\n",
    "            std_daily_return = np.std(daily_returns) #return variability per day\n",
    "\n",
    "            #annualised_return = (1 + mean_cumulative_return_for_portfolio)**(252/t) - 1\n",
    "            #annualised_volatility = std_cumulative_return * np.sqrt(252/t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            final_portfolio_values = portfolio_sims[-1, :]  # Final values across all simulations\n",
    "            initial_value = 100  # Your initial portfolio value\n",
    "            holding_period_years = t / 252  # Convert days to years\n",
    "\n",
    "            # Calculate annualized returns \n",
    "            return_for_period = (final_portfolio_values - initial_value) / initial_value\n",
    "            annualised_return = (final_portfolio_values / initial_value) ** (1 / holding_period_years) - 1\n",
    "            #annualised_return = (1 + return_for_period) ** (1/holding_period_years) - 1\n",
    "            mean_return = np.mean(return_for_period)\n",
    "            mean_annual_return = np.mean(annualised_return)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #sharpe ratio\n",
    "            rf_daily = rf_annual / 252\n",
    "            #rf_cumulative = (1 + rf_annual) ** (daily_returns.shape[0] / 252) - 1\n",
    "            sharpe_daily = (mean_daily_return_for_portfolio - rf_daily) / std_daily_return\n",
    "            #sharpe_cumulative = (mean_cumulative_return_for_portfolio - rf_cumulative)  / std_cumulative_return\n",
    "            sharpe_annual = sharpe_daily * np.sqrt(252)\n",
    "            #sharpe_period = \n",
    "            #sharpe_cumulative_annual = sharpe_cumulative * np.sqrt(252)\n",
    "\n",
    "            #Sortino ratio\n",
    "            # downside_returns = np.minimum(0, daily_returns - rf_daily)\n",
    "\n",
    "            # with np.errstate(over='ignore'):  # Suppress the warning\n",
    "            #     squared_returns = np.square(downside_returns, dtype=np.float64)\n",
    "            #     downside_std = np.sqrt(np.mean(squared_returns))\n",
    "\n",
    "            # if np.isclose(downside_std, 0) or np.isnan(downside_std):\n",
    "            #     sortino_ratio = np.nan  # or set to a default value\n",
    "            # else:\n",
    "            #     sortino_ratio = (mean_daily_return_for_portfolio - rf_daily) / downside_std\n",
    "\n",
    "            #sortino_annual = sortino_ratio * np.sqrt(252)\n",
    "\n",
    "            #VaR:\n",
    "            last_period_returns = portfolio_sims[-1:]\n",
    "            initial_portfolio_value = 100\n",
    "            portfolio_returns = (last_period_returns - initial_portfolio_value) / initial_portfolio_value\n",
    "            VaR = np.percentile(portfolio_returns, 5)\n",
    "            VaR_final = abs(VaR) * initial_portfolio_value\n",
    "            #CVaR\n",
    "            worst_losses = portfolio_returns[portfolio_returns <= VaR]\n",
    "            CVaR_final = abs(worst_losses.mean()) * initial_portfolio_value\n",
    "\n",
    "            VaR_final, CVaR_final = calculate_var_cvar(daily_returns, \n",
    "                                                 confidence_level=0.05, \n",
    "                                                 initial_value=initial_portfolio_value)\n",
    "\n",
    "\n",
    "            stat_results = pd.DataFrame({'annualised_return': [mean_annual_return],\n",
    "                                     'mean_period_return': [mean_return],\n",
    "                                     #'annualised_volatility': [annualised_volatility],\n",
    "                                     #'std_daily_return': [std_daily_return],\n",
    "                                     #'sharpe_daily': [sharpe_daily],\n",
    "                                     #'sharpe_cumulative': [sharpe_cumulative],\n",
    "                                     'sharpe_annualized': [sharpe_annual],\n",
    "                                     'VaR': [VaR_final],\n",
    "                                     #'CVaR': [CVaR_final],\n",
    "                                     #'sortino': [sortino_ratio],\n",
    "                                     #'sortino_annualized': [sortino_annual]\n",
    "                                     })\n",
    "\n",
    "            subset_statistics_df = pd.concat([subset_statistics_df, stat_results])\n",
    "\n",
    "    subset_statistics_df = subset_statistics_df.reset_index(drop=True)\n",
    "\n",
    "    #RUN THE NORMALITY TEST\n",
    "    results_normality_test = {}\n",
    "    for col in subset_statistics_df.columns:\n",
    "        stat, p_value = normaltest(subset_statistics_df[col])\n",
    "        results_normality_test[col] = {'statistic': stat, 'p_value': p_value}\n",
    "\n",
    "    normality_results_df = pd.DataFrame(results_normality_test).T\n",
    "    normality_results_df['normal'] = normality_results_df['p_value'] > 0.05\n",
    "\n",
    "\n",
    "    return simulations_results_dict, subset_statistics_df, normality_results_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
