{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "\n",
    "from functions import sharpe_ratio_calculation, generate_rand_portfolios, select_top_five, join_stocks_crypto, test_for_silhouette_score, run_clustering_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stocks = pd.read_csv('stocks_data_filled.csv',index_col='Date')\n",
    "cryptos_df = pd.read_csv('cryptos_data.csv', index_col='Date')\n",
    "\n",
    "joined_df = join_stocks_crypto(cryptos_df, df_all_stocks, mode = 'stocks_left') #mode - either do left with crypto and fill NA for stocks or do left on stocks and leave out some dates for cryptos\n",
    "joined_df.index = pd.to_datetime(joined_df.index)\n",
    "joined_df = joined_df.bfill()\n",
    "\n",
    "joined_df_weekly = joined_df.resample('W').last() #try aggregating on a weekly level\n",
    "joined_df_3days = joined_df.resample('3D').last()# aggregating on a twice per week basis to arrive at the sweet spot of that 250 (1 year) timeseries length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Portfolios generation\n",
    "tickers = list(df_all_stocks.columns)\n",
    "\n",
    "random.seed(42)\n",
    "random_portfolios = generate_rand_portfolios(n_reps=1000, n_stocks=15, tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select top five sharpe ratio portfolios from a portfolio\n",
    "sharpe_ratio = sharpe_ratio_calculation(df_all_stocks, rf_rate_annual = 0.02)\n",
    "top_five_dict = select_top_five(random_portfolios, metric=sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimize Traditional Portfolios\n",
    "# for i in range(999, 1001):\n",
    "\n",
    "#     print('Doing', i)\n",
    "\n",
    "#     top_five_sets = dict(itertools.islice(top_five_dict.items(), i, i+1))\n",
    "#     results = run_min_variance(df_all_stocks, top_five_sets, min_weight_for_top_five=0.05)  #TRY DIFFERENT WEIGHTS FOR top_five\n",
    "#     with open(f\"min_variance_portfolio_jsons/my_dict{i}.json\", \"w\") as f:\n",
    "#         json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassemble the results of the optimization - jsons\n",
    "# min_var_portfolios = dict()\n",
    "# for i in range(1,1000):\n",
    "#     with open(f'min_variance_portfolio_jsons/my_dict{i}.json') as f:\n",
    "#         port = json.load(f)\n",
    "#         min_var_portfolios.update(port)\n",
    "\n",
    "# with open(f\"full_optimized_min_variance.json\", \"w\") as f:\n",
    "#     json.dump(min_var_portfolios, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_clusters_list = [4,5,6,7]\n",
    "# linkage_list=['single', 'average', 'complete']\n",
    "# #window_sizes = [3,7,10,14,21,30,60]\n",
    "# window_sizes = [21,30,60]\n",
    "\n",
    "\n",
    "# def run_clustering_evaluation(df, window_sizes, method, moving_average=True, return_mode='arithmetic', df_input_name='DFWASNOTSPECIFIED'):\n",
    "\n",
    "#     for w_size in window_sizes:\n",
    "\n",
    "#         #return_mode = 'arithmetic'\n",
    "#         #n_init = 3\n",
    "#         #center = True\n",
    "#         if moving_average:\n",
    "#             df = df.rolling(window=w_size, center=True).mean()\n",
    "\n",
    "#             smoothing = 'moving_average'\n",
    "#         else:\n",
    "#             smoothing = 'no_smoothing'\n",
    "\n",
    "#         if len(df) < 150 and w_size > 30:\n",
    "#             continue\n",
    "#         silhouette_df = test_for_silhouette_score(df, n_clusters_list, method=method, return_mode=return_mode, n_init=3, linkage_list=linkage_list)\n",
    "\n",
    "#         silhouette_df['return_mode'] = return_mode\n",
    "#         silhouette_df['n_init'] = 3\n",
    "#         silhouette_df['smoothing'] = smoothing\n",
    "#         silhouette_df['window_size/span'] = w_size\n",
    "\n",
    "#         silhouette_df.to_csv(f'silhouette_dfs/{method}_{smoothing}_{return_mode}_windowsize-{w_size}_{df_input_name}.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_input_name = input('Put in the name of the df mode you are running for: ')\n",
    "# for return_mode in ['arithmetic', 'geometric']:\n",
    "#     run_clustering_evaluation(joined_df_3days, window_sizes, method='kmeans', moving_average=True, return_mode=return_mode, df_input_name=df_input_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "def label_balance(df_dict:dict, window, method, return_mode, n_clus, linkage):\n",
    "\n",
    "    # Suppress all warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "    df_name = list(df_dict.keys())[0]\n",
    "\n",
    "    df_smooth = df_dict[df_name].rolling(window=window, center=True).mean()\n",
    "    _, tickers_with_labels, _, _ = run_clustering_model(df_smooth, n_clus=n_clus, model_name=method, linkage=linkage, return_mode=return_mode, n_init=3)\n",
    "\n",
    "    res = pd.DataFrame(list(tickers_with_labels.items()), columns=['ticker', 'label'])\n",
    "    out = res.groupby('label').count()\n",
    "\n",
    "    #if not ((out['ticker'] / len(joined_df.columns)) >= 0.6).any():\n",
    "    max_percentage_per_cluster = (out['ticker'] / len(joined_df.columns)).max()\n",
    "    min_percentage_per_cluster = (out['ticker'] / len(joined_df.columns)).min()\n",
    "    min_max_delta = round(max_percentage_per_cluster - min_percentage_per_cluster, 4)\n",
    "        #print(f'Window - {window},method - {method},return mode - {return_mode} \\nMax {round(max_percentage_per_cluster * 100, 2)} % of observations per cluster  \\nMin {round(min_percentage_per_cluster*100, 2)} % of observations per cluster')\n",
    "\n",
    "    # out['return_mode'] = return_mode\n",
    "    # out['window_size'] = window\n",
    "    # out['method'] = method\n",
    "    # out['clusters'] = n_clus\n",
    "    # out['linkage'] = linkage\n",
    "    # out['df_mode'] = df_name\n",
    "    if method != 'ahc':\n",
    "        linkage = 'not_applicable'\n",
    "\n",
    "    output = {'return_mode': [return_mode], \n",
    "              'window_size': [window], \n",
    "              'method': [method],\n",
    "              'linkage': [linkage], \n",
    "              'df_mode': [df_name], \n",
    "               'min_per_cluster': [round(min_percentage_per_cluster, 4)],\n",
    "              'max_per_cluster': [round(max_percentage_per_cluster, 4)],\n",
    "              'min_max_delta': [min_max_delta]}\n",
    "    \n",
    "    output_df_one_row = pd.DataFrame(output)\n",
    "    \n",
    "\n",
    "    return output_df_one_row #out, max_percentage_per_cluster, min_percentage_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS FOR FULL DF\n",
    "\n",
    "method_loop = 'ahc'\n",
    "return_mode_loop = 'geometric'\n",
    "n_clus_loop = 4\n",
    "\n",
    "df_with_label_balance = pd.DataFrame()\n",
    "for df_dict in [{'weekly': joined_df_weekly}, {'3day': joined_df_3days}, {'full': joined_df}]:\n",
    "    for linkage in ['single', 'complete', 'average']:\n",
    "        for w in [3,7,10,14,30]:\n",
    "            output = label_balance(df_dict, w, method_loop, return_mode_loop, n_clus=n_clus_loop, linkage=linkage)\n",
    "            df_with_label_balance = pd.concat([df_with_label_balance, output])\n",
    "\n",
    "df_with_label_balance.reset_index(inplace=True)\n",
    "\n",
    "df_with_label_balance.to_csv(f'balance_test_results_{method_loop}_{return_mode_loop}_{n_clus_loop}clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
