{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "\n",
    "from functions import sharpe_ratio_calculation, generate_rand_portfolios, select_top_five, join_stocks_crypto, distance_matrix_calc, run_min_variance, run_clustering_model, test_for_silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_stocks = pd.read_csv('stocks_data.csv',index_col='Date')\n",
    "cryptos_df = pd.read_csv('cryptos_data.csv', index_col='Date')\n",
    "\n",
    "joined_df = join_stocks_crypto(cryptos_df, df_all_stocks, mode = 'stocks_left') #mode - either do left with crypto and fill NA for stocks or do left on stocks and leave out some dates for cryptos\n",
    "joined_df.index = pd.to_datetime(joined_df.index)\n",
    "\n",
    "joined_df_weekly = joined_df.resample('W').last() #try aggregating on a weekly level\n",
    "\n",
    "joined_df_3days = joined_df.resample('3D').last()# aggregating on a twice per week basis to arrive at the sweet spot of that 250 (1 year) timeseries length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Portfolios generation\n",
    "tickers = list(df_all_stocks.columns)\n",
    "\n",
    "random.seed(42)\n",
    "random_portfolios = generate_rand_portfolios(n_reps=1000, n_stocks=15, tickers=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select top five sharpe ratio portfolios from a portfolio\n",
    "sharpe_ratio = sharpe_ratio_calculation(df_all_stocks, rf_rate_annual = 0.02)\n",
    "top_five_dict = select_top_five(random_portfolios, metric=sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(999, 1001):\n",
    "\n",
    "#     print('Doing', i)\n",
    "\n",
    "#     top_five_sets = dict(itertools.islice(top_five_dict.items(), i, i+1))\n",
    "#     results = run_min_variance(df_all_stocks, top_five_sets, min_weight_for_top_five=0.05)  #TRY DIFFERENT WEIGHTS FOR top_five\n",
    "#     with open(f\"min_variance_portfolio_jsons/my_dict{i}.json\", \"w\") as f:\n",
    "#         json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassemble the results of the generation\n",
    "# min_var_portfolios = dict()\n",
    "# for i in range(1,1000):\n",
    "#     with open(f'min_variance_portfolio_jsons/my_dict{i}.json') as f:\n",
    "#         port = json.load(f)\n",
    "#         min_var_portfolios.update(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_list = [4,5,6]\n",
    "linkage_list=['single', 'average', 'complete']\n",
    "window_sizes = [3,7,10,14,21,30,60]\n",
    "\n",
    "\n",
    "def run_clustering_evaluation(df, window_sizes, method, moving_average=True):\n",
    "\n",
    "    df_input_name = input('Put in the name of the df mode you are running for: ')\n",
    "\n",
    "    for w_size in window_sizes:\n",
    "\n",
    "        #return_mode = 'arithmetic'\n",
    "        #n_init = 3\n",
    "        #center = True\n",
    "        if moving_average:\n",
    "            df = joined_df.rolling(window=w_size, center=True).mean()\n",
    "\n",
    "            smoothing = 'moving_average'\n",
    "        else:\n",
    "            smoothing = 'no_smoothing'\n",
    "\n",
    "\n",
    "        silhouette_df = test_for_silhouette_score(df, n_clusters_list, method=method, return_mode='arithmetic', n_init=3, linkage_list=linkage_list)\n",
    "\n",
    "        silhouette_df['return_mode'] = 'arithmetic'\n",
    "        silhouette_df['n_init'] = 3\n",
    "        silhouette_df['smoothing'] = smoothing\n",
    "        silhouette_df['window_size/span'] = w_size\n",
    "\n",
    "        silhouette_df.to_csv(f'silhouette_dfs/{method}_{smoothing}_windowsize-{w_size}_{df_input_name}.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_clustering_evaluation(joined_df_weekly, window_sizes, method='kmeans', moving_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'silhouette_dfs/'\n",
    "\n",
    "# dfs_456_sil = pd.DataFrame()\n",
    "# dfs_all_sil = pd.DataFrame()\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     file_path = os.path.join(folder_path, filename)\n",
    "#     if '456' in filename: \n",
    "#         df = pd.read_csv(file_path)\n",
    "#         dfs_456_sil = pd.concat([dfs_456_sil, df], axis=0)\n",
    "#     else:\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         df['filename'] = filename\n",
    "#         dfs_all_sil = pd.concat([dfs_all_sil, df], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
