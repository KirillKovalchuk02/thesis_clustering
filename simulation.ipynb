{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d97807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import join_stocks_crypto, generate_rand_portfolios\n",
    "from functions_post_clustering import dunn_bonferroni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\ipykernel_14316\\3916790673.py:20: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  returns_all_24 = joined_df_24.pct_change().dropna()\n"
     ]
    }
   ],
   "source": [
    "#GET THE DATA IN\n",
    "\n",
    "#Main part data 2022-2023\n",
    "df_all_stocks = pd.read_csv('stocks_data_FINAL.csv',index_col='Date')\n",
    "df_all_stocks.index = pd.to_datetime(df_all_stocks.index)\n",
    "df_all_stocks.index = df_all_stocks.index.strftime('%Y-%m-%d')\n",
    "\n",
    "cryptos_df = pd.read_csv('cryptos_data_new.csv', index_col='timestamp')\n",
    "joined_df = join_stocks_crypto(cryptos_df, df_all_stocks, mode = 'stocks_left')\n",
    "joined_df.index = pd.to_datetime(joined_df.index)\n",
    "returns_all = joined_df.pct_change().dropna()\n",
    "\n",
    "tickers_to_select = list(joined_df.columns)\n",
    "\n",
    "#Out-of-sample data 2024:\n",
    "df_stocks_24 = pd.read_csv('stocks_data_out_sample_2024_FINAL.csv',index_col='Date')\n",
    "cryptos_df_24 = pd.read_csv('cryptos_data_out_sample_2024.csv', index_col='timestamp')\n",
    "joined_df_24 = join_stocks_crypto(cryptos_df_24, df_stocks_24, mode = 'stocks_left')\n",
    "joined_df_24 = joined_df_24[tickers_to_select]\n",
    "joined_df_24.index = pd.to_datetime(joined_df_24.index)\n",
    "returns_all_24 = joined_df_24.pct_change().dropna()\n",
    "\n",
    "\n",
    "tickers = list(df_all_stocks.columns)\n",
    "\n",
    "random.seed(42)\n",
    "random_portfolios = generate_rand_portfolios(n_reps=1000, n_stocks=15, tickers=tickers)\n",
    "\n",
    "with open('equalw_sets_new.json') as f:\n",
    "    equalw_sets = json.load(f)\n",
    "\n",
    "with open('mdr_reoptimized_sets_new_bounds.json') as f:\n",
    "    mdr_reoptimized_sets = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b0a8d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average df: 11.13, Min df: 5.71\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from functions_post_clustering import estimate_t_df_for_portfolio\n",
    "import math\n",
    "\n",
    "df_estimate = math.floor(estimate_t_df_for_portfolio(returns_all))\n",
    "print(df_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b740f",
   "metadata": {},
   "source": [
    "SIMULATE AND EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ea3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL CLAUDE GENERATION CODE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mstats, normaltest\n",
    "import random\n",
    "\n",
    "\n",
    "def run_simulation(portfolio_dict, returns_for_portfolio, n_sims=100, t=100, \n",
    "                  distribution_model='multivar_norm', plot=False, initialPortfolio=100, \n",
    "                  winsorize=False, winsorize_limits=(0.01, 0.01), use_log_returns=False):\n",
    "    returns_for_portfolio = returns_for_portfolio[list(portfolio_dict.keys())]\n",
    "    \n",
    "    if winsorize:\n",
    "        winsorized_returns = returns_for_portfolio.copy()\n",
    "        \n",
    "        for col in winsorized_returns.columns:\n",
    "            winsorized_returns[col] = mstats.winsorize(winsorized_returns[col], limits=winsorize_limits)\n",
    "        \n",
    "        returns_for_portfolio = winsorized_returns\n",
    "\n",
    "    # Convert to log returns if specified (for consistency with out-of-sample)\n",
    "    if use_log_returns:\n",
    "        invalid_mask = returns_for_portfolio <= -1.0\n",
    "        if invalid_mask.any().any():\n",
    "            log_compatible_returns = returns_for_portfolio.copy()\n",
    "            log_compatible_returns[invalid_mask] = -0.99\n",
    "            log_returns = np.log(1 + log_compatible_returns)\n",
    "            returns_for_portfolio = log_returns\n",
    "        else:\n",
    "            returns_for_portfolio = np.log(1 + returns_for_portfolio)\n",
    "\n",
    "    mean_returns = returns_for_portfolio.mean()\n",
    "    cov_matrix = returns_for_portfolio.cov()\n",
    "\n",
    "    weights = np.array([v for _, v in portfolio_dict.items()])\n",
    "\n",
    "    meanM = np.tile(mean_returns, (t, 1))  # Shape: (T, n_assets)\n",
    "\n",
    "    portfolio_sims = np.zeros((t, n_sims))\n",
    "\n",
    "    L = np.linalg.cholesky(cov_matrix)  # Cholesky decomposition\n",
    "\n",
    "    for sim in range(n_sims):\n",
    "        if distribution_model == 'bootstrap':\n",
    "            sampled_returns = returns_for_portfolio.bfill().sample(n=t, replace=True).values\n",
    "            portfolio_returns = sampled_returns @ weights\n",
    "        elif distribution_model in ['multivar_norm', 'multivar_t']:\n",
    "            if distribution_model == 'multivar_norm':\n",
    "                Z = np.random.normal(size=(t, len(portfolio_dict)))  # Shape: (T, n_assets)\n",
    "                daily_returns = meanM + Z @ L.T  # Shape: (T, n_assets)\n",
    "            elif distribution_model == 'multivar_t':\n",
    "                df = 11  # degrees of freedom\n",
    "                Z = np.random.normal(size=(t, len(portfolio_dict)))\n",
    "                chi2 = np.random.chisquare(df, size=(t, 1))\n",
    "                Z_t = Z / np.sqrt(chi2 / df)  # now Z_t has t-distributed marginals\n",
    "\n",
    "                daily_returns = meanM + Z_t @ L.T\n",
    "\n",
    "            portfolio_returns = daily_returns @ weights  # Shape: (T,)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # Convert back from log returns if necessary\n",
    "        if use_log_returns:\n",
    "            portfolio_returns = np.exp(portfolio_returns) - 1\n",
    "        \n",
    "        portfolio_sims[:, sim] = np.cumprod(1 + portfolio_returns) * initialPortfolio\n",
    "\n",
    "    return portfolio_sims\n",
    "\n",
    "def out_of_sample_analysis(portfolio, return_data, start_date, period=126, \n",
    "                          calculate_metrics=True, plot_results=False, risk_free_rate=0.02, \n",
    "                          use_log_returns=False):\n",
    "    \n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "    \n",
    "    results = {}\n",
    "    tickers = list(portfolio.keys())\n",
    "    weights = np.array([portfolio[ticker] for ticker in tickers])\n",
    "    \n",
    "    # Validate that all tickers exist in the data\n",
    "    missing_tickers = [ticker for ticker in tickers if ticker not in return_data.columns]\n",
    "    if missing_tickers:\n",
    "        raise ValueError(f\"The following tickers are missing from return_data: {missing_tickers}\")\n",
    "    \n",
    "    # Extract relevant data\n",
    "    return_subset = return_data[tickers].copy()\n",
    "    return_subset = return_subset.sort_index()\n",
    "    oos_data = return_subset[return_subset.index >= start_date]\n",
    "    \n",
    "    if oos_data.empty:\n",
    "        raise ValueError(f\"No data available after start_date: {start_date}\")\n",
    "    \n",
    "    # Get the specified period of data\n",
    "    period_data = oos_data.iloc[:period]\n",
    "    returns = period_data.copy()\n",
    "\n",
    "    if use_log_returns:\n",
    "        # Handle potential invalid values for log transformation\n",
    "        invalid_mask = returns <= -1.0\n",
    "        if invalid_mask.any().any():\n",
    "            print(f\"Warning: Found {invalid_mask.sum().sum()} invalid values for log transformation. Capping at -99%.\")\n",
    "            log_compatible_returns = returns.copy()\n",
    "            log_compatible_returns[invalid_mask] = -0.99  # Cap extreme losses\n",
    "            returns_for_portfolio = log_compatible_returns\n",
    "        else:\n",
    "            returns_for_portfolio = returns\n",
    "        \n",
    "        # CORRECTED: Calculate portfolio simple returns first\n",
    "        portfolio_simple_returns = returns_for_portfolio.dot(weights)\n",
    "        \n",
    "        # Handle edge case where portfolio returns <= -1\n",
    "        portfolio_invalid_mask = portfolio_simple_returns <= -1.0\n",
    "        if portfolio_invalid_mask.any():\n",
    "            print(f\"Warning: Found {portfolio_invalid_mask.sum()} invalid portfolio returns for log transformation. Capping at -99%.\")\n",
    "            portfolio_simple_returns[portfolio_invalid_mask] = -0.99\n",
    "        \n",
    "        # Convert portfolio simple returns to log returns\n",
    "        portfolio_log_returns = np.log(1 + portfolio_simple_returns)\n",
    "        \n",
    "        # Use simple returns for most calculations\n",
    "        portfolio_returns = portfolio_simple_returns\n",
    "        \n",
    "        # Calculate cumulative returns using log returns (more numerically stable)\n",
    "        cumulative_returns = np.exp(portfolio_log_returns.cumsum()) - 1\n",
    "        \n",
    "    else:\n",
    "        # Standard simple returns approach\n",
    "        portfolio_returns = returns.dot(weights)\n",
    "        cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "\n",
    "    # Store basic results\n",
    "    results = {\n",
    "        'returns': portfolio_returns,\n",
    "        'cumulative_returns': cumulative_returns,\n",
    "        'period_data': period_data,\n",
    "        'weights': weights,\n",
    "        'tickers': tickers\n",
    "    }\n",
    "    \n",
    "    if calculate_metrics:\n",
    "        # Calculate performance metrics\n",
    "        total_return = cumulative_returns.iloc[-1] if len(cumulative_returns) > 0 else 0\n",
    "        trading_days = len(portfolio_returns)\n",
    "        \n",
    "        # Annualization factor based on actual trading days\n",
    "        annual_factor = 252 / trading_days if trading_days > 0 else 1\n",
    "        annualized_return = (1 + total_return) ** annual_factor - 1\n",
    "\n",
    "        # Volatility calculations\n",
    "        volatility = portfolio_returns.std() * np.sqrt(252) if len(portfolio_returns) > 1 else 0\n",
    "        \n",
    "        # Sharpe ratio calculations\n",
    "        sharpe_ratio = (annualized_return - risk_free_rate) / volatility if volatility > 0 else 0\n",
    "        \n",
    "        # Non-annualized Sharpe ratio for the actual period\n",
    "        rf_period = risk_free_rate * trading_days / 252\n",
    "        volatility_period = portfolio_returns.std() * np.sqrt(trading_days) if len(portfolio_returns) > 1 else 0\n",
    "        sharpe_period = (total_return - rf_period) / volatility_period if volatility_period > 0 else 0\n",
    "        \n",
    "        # Risk metrics\n",
    "        var_95 = abs(portfolio_returns.quantile(0.05)) * 100 if len(portfolio_returns) > 0 else 0\n",
    "        var_99 = abs(portfolio_returns.quantile(0.01)) * 100 if len(portfolio_returns) > 0 else 0\n",
    "        \n",
    "        # Maximum drawdown calculation\n",
    "        rolling_max = cumulative_returns.expanding().max()\n",
    "        drawdown = (cumulative_returns - rolling_max) / (1 + rolling_max)\n",
    "        max_drawdown = abs(drawdown.min()) * 100 if len(drawdown) > 0 else 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        positive_days = (portfolio_returns > 0).sum() if len(portfolio_returns) > 0 else 0\n",
    "        win_rate = positive_days / len(portfolio_returns) * 100 if len(portfolio_returns) > 0 else 0\n",
    "        \n",
    "        results.update({\n",
    "            'mean_period_return': total_return,\n",
    "            'annualised_return': annualized_return,\n",
    "            'volatility': volatility,\n",
    "            'sharpe_annualized': sharpe_ratio,\n",
    "            'sharpe_period': sharpe_period,\n",
    "            'var_95': var_95,\n",
    "        })\n",
    "    \n",
    "  \n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_diversification_ratio(portfolio_dict, return_df):\n",
    "    try:\n",
    "        # Get the assets in the portfolio\n",
    "        assets = list(portfolio_dict.keys())\n",
    "        weights = np.array([portfolio_dict[asset] for asset in assets])\n",
    "        \n",
    "        # Filter return data to only include portfolio assets\n",
    "        portfolio_returns = return_df[assets].dropna()\n",
    "        \n",
    "        if len(portfolio_returns) < 30:  # Need minimum data for reliable volatility estimates\n",
    "            return np.nan\n",
    "        \n",
    "        # Calculate individual asset volatilities (annualized)\n",
    "        individual_volatilities = portfolio_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Calculate weighted average of individual volatilities\n",
    "        weighted_avg_volatility = np.sum(weights * individual_volatilities)\n",
    "        \n",
    "        # Calculate portfolio returns\n",
    "        portfolio_return_series = (portfolio_returns * weights).sum(axis=1)\n",
    "        \n",
    "        # Calculate portfolio volatility (annualized)\n",
    "        portfolio_volatility = portfolio_return_series.std() * np.sqrt(252)\n",
    "        \n",
    "        # Calculate diversification ratio\n",
    "        if portfolio_volatility > 0:\n",
    "            diversification_ratio = weighted_avg_volatility / portfolio_volatility\n",
    "            return diversification_ratio\n",
    "        else:\n",
    "            return np.nan\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating diversification ratio: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Add this helper function\n",
    "def calculate_cvar(returns, confidence_level=0.05):\n",
    "    \"\"\"\n",
    "    Calculate Conditional Value at Risk (CVaR) - the expected loss beyond VaR\n",
    "    \"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    var_threshold = np.percentile(returns, confidence_level * 100)\n",
    "    tail_losses = returns[returns <= var_threshold]\n",
    "    \n",
    "    if len(tail_losses) == 0:\n",
    "        return abs(var_threshold) * 100\n",
    "    \n",
    "    cvar = abs(np.mean(tail_losses)) * 100\n",
    "    return cvar\n",
    "\n",
    "\n",
    "def simulate_evaluate_portfolio_subset(portfolios_subset, return_df, n_sims=100, t=100, \n",
    "                                      distribution_model='multivar_norm', rf_annual=0.04, \n",
    "                                      seed=30, winsorize=False, winsorize_limits=(0.01, 0.01),\n",
    "                                      use_log_returns=False):\n",
    "    \n",
    "    simulations_results_dict = dict()\n",
    "    subset_statistics_df = pd.DataFrame()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for i, portfolio_dict in portfolios_subset.items():\n",
    "        # Making sure weights sum up to 1\n",
    "        total = sum(portfolio_dict.values())\n",
    "        portfolio_dict = {k: v / total for k, v in portfolio_dict.items()}\n",
    "        \n",
    "        # Calculate diversification ratio for this portfolio\n",
    "        diversification_ratio = calculate_diversification_ratio(portfolio_dict, return_df)\n",
    "\n",
    "        if distribution_model == 'out_sample_direct':\n",
    "            # Out-of-sample analysis\n",
    "            res = out_of_sample_analysis(\n",
    "                portfolio_dict, \n",
    "                return_df, \n",
    "                '2024-01-01', \n",
    "                period=t, \n",
    "                calculate_metrics=True, \n",
    "                risk_free_rate=rf_annual, \n",
    "                plot_results=False, \n",
    "                use_log_returns=use_log_returns\n",
    "            )\n",
    "            \n",
    "            mean_annual_return = res['annualised_return']\n",
    "            mean_return = res['mean_period_return']\n",
    "            sharpe_annual = res['sharpe_annualized']\n",
    "            sharpe_period = res['sharpe_period']\n",
    "            \n",
    "            # Bootstrap period VaR/CVaR for consistency with Monte Carlo\n",
    "            portfolio_returns = res['returns']  # Daily returns over the period\n",
    "            n_bootstrap = 500\n",
    "            bootstrap_period_returns = []\n",
    "            \n",
    "            np.random.seed(seed)  # For reproducibility\n",
    "            for _ in range(n_bootstrap):\n",
    "                # Sample with replacement from actual daily returns\n",
    "                sampled_returns = np.random.choice(portfolio_returns, size=len(portfolio_returns), replace=True)\n",
    "                # Calculate cumulative return for this bootstrap sample\n",
    "                bootstrap_period_return = (1 + sampled_returns).prod() - 1\n",
    "                bootstrap_period_returns.append(bootstrap_period_return)\n",
    "            \n",
    "            bootstrap_period_returns = np.array(bootstrap_period_returns)\n",
    "            \n",
    "            # Calculate period VaR and CVaR from bootstrap\n",
    "            period_var_95 = abs(np.percentile(bootstrap_period_returns, 5)) * 100\n",
    "            period_cvar_95 = calculate_cvar(bootstrap_period_returns, confidence_level=0.05)\n",
    "            \n",
    "            # Annualize VaR/CVaR for cross-period comparison\n",
    "            actual_period_days = len(portfolio_returns)\n",
    "            annualized_var = period_var_95 * np.sqrt(252 / actual_period_days)\n",
    "            annualized_cvar = period_cvar_95 * np.sqrt(252 / actual_period_days)\n",
    "            \n",
    "            # Calculate annualized volatility for consistency\n",
    "            daily_volatility = portfolio_returns.std()\n",
    "            annualized_volatility = daily_volatility * np.sqrt(252)\n",
    "\n",
    "            stat_results = pd.DataFrame({\n",
    "                'annualised_return': [mean_annual_return],\n",
    "                'mean_period_return': [mean_return],\n",
    "                'sharpe_annualized': [sharpe_annual],\n",
    "                'sharpe_period': [sharpe_period],\n",
    "                'period_VaR': [period_var_95],\n",
    "                'period_CVaR': [period_cvar_95],\n",
    "                'annualized_VaR': [annualized_var],\n",
    "                'annualized_CVaR': [annualized_cvar],\n",
    "                'annualized_volatility': [annualized_volatility],\n",
    "            })\n",
    "            \n",
    "            subset_statistics_df = pd.concat([subset_statistics_df, stat_results])\n",
    "            \n",
    "        else:\n",
    "            # Monte Carlo simulation\n",
    "            portfolio_sims = run_simulation(\n",
    "                portfolio_dict, \n",
    "                return_df[list(portfolio_dict.keys())], \n",
    "                n_sims=n_sims, \n",
    "                t=t, \n",
    "                distribution_model=distribution_model, \n",
    "                plot=False, \n",
    "                winsorize=winsorize, \n",
    "                winsorize_limits=winsorize_limits,\n",
    "                use_log_returns=use_log_returns\n",
    "            )\n",
    "\n",
    "            simulations_results_dict[i] = portfolio_sims\n",
    "            actual_period_days = t\n",
    "            \n",
    "            # Calculate total period returns across all simulations\n",
    "            initial_values = portfolio_sims[0, :]  # Should be 100 for all simulations\n",
    "            final_values = portfolio_sims[-1, :]   # Final portfolio values\n",
    "            total_returns_array = (final_values - initial_values) / initial_values\n",
    "            \n",
    "            # Calculate daily returns for volatility calculations\n",
    "            daily_returns = (portfolio_sims[1:, :] - portfolio_sims[:-1, :]) / portfolio_sims[:-1, :]\n",
    "            \n",
    "            # Calculate period VaR and CVaR from total returns across simulations\n",
    "            period_var_95 = abs(np.percentile(total_returns_array, 5)) * 100\n",
    "            period_cvar_95 = calculate_cvar(total_returns_array, confidence_level=0.05)\n",
    "            \n",
    "            # Annualize VaR/CVaR for cross-period comparison\n",
    "            annualized_var = period_var_95 * np.sqrt(252 / actual_period_days)\n",
    "            annualized_cvar = period_cvar_95 * np.sqrt(252 / actual_period_days)\n",
    "            \n",
    "            # Calculate metrics for each simulation path\n",
    "            all_stats = []\n",
    "            \n",
    "            for sim in range(n_sims):\n",
    "                # Get daily returns for this simulation\n",
    "                sim_daily_returns = daily_returns[:, sim]\n",
    "                total_return = total_returns_array[sim]\n",
    "                \n",
    "                # Annualization factor based on full simulation period\n",
    "                annual_factor = 252 / actual_period_days\n",
    "                annualized_return = (1 + total_return) ** annual_factor - 1\n",
    "                \n",
    "                # Calculate volatility (annualized) from daily returns\n",
    "                daily_volatility = sim_daily_returns.std()\n",
    "                annualized_volatility = daily_volatility * np.sqrt(252)\n",
    "                \n",
    "                # Calculate Sharpe ratios\n",
    "                sharpe_annual = (annualized_return - rf_annual) / annualized_volatility if annualized_volatility > 0 else 0\n",
    "                \n",
    "                # Period Sharpe ratio - use consistent period length\n",
    "                rf_period = rf_annual * actual_period_days / 252\n",
    "                period_volatility = daily_volatility * np.sqrt(actual_period_days)\n",
    "                sharpe_period = (total_return - rf_period) / period_volatility if period_volatility > 0 else 0\n",
    "                \n",
    "                all_stats.append({\n",
    "                    'annualised_return': annualized_return,\n",
    "                    'mean_period_return': total_return,\n",
    "                    'sharpe_annualized': sharpe_annual,\n",
    "                    'sharpe_period': sharpe_period,\n",
    "                    'annualized_volatility': annualized_volatility\n",
    "                })\n",
    "            \n",
    "            # Convert to DataFrame and calculate averages\n",
    "            sim_stats_df = pd.DataFrame(all_stats)\n",
    "            \n",
    "            mean_annual_return = sim_stats_df['annualised_return'].mean()\n",
    "            mean_return = sim_stats_df['mean_period_return'].mean()\n",
    "            mean_sharpe_annual = sim_stats_df['sharpe_annualized'].mean()\n",
    "            mean_sharpe_period = sim_stats_df['sharpe_period'].mean()\n",
    "            mean_volatility = sim_stats_df['annualized_volatility'].mean()\n",
    "            \n",
    "            stat_results = pd.DataFrame({\n",
    "                'annualised_return': [mean_annual_return],\n",
    "                'mean_period_return': [mean_return],\n",
    "                'sharpe_annualized': [mean_sharpe_annual],\n",
    "                'sharpe_period': [mean_sharpe_period],\n",
    "                'period_VaR': [period_var_95],\n",
    "                'period_CVaR': [period_cvar_95],\n",
    "                'annualized_VaR': [annualized_var],\n",
    "                'annualized_CVaR': [annualized_cvar],\n",
    "                'annualized_volatility': [mean_volatility],\n",
    "            })\n",
    "\n",
    "            subset_statistics_df = pd.concat([subset_statistics_df, stat_results])\n",
    "\n",
    "    subset_statistics_df = subset_statistics_df.reset_index(drop=True)\n",
    "\n",
    "    # Run normality test on all metrics\n",
    "    results_normality_test = {}\n",
    "    for col in subset_statistics_df.columns:\n",
    "        if subset_statistics_df[col].dtype in ['float64', 'int64']:  # Only test numeric columns\n",
    "            stat, p_value = normaltest(subset_statistics_df[col])\n",
    "            results_normality_test[col] = {'statistic': stat, 'p_value': p_value}\n",
    "\n",
    "    normality_results_df = pd.DataFrame(results_normality_test).T\n",
    "    normality_results_df['normal'] = normality_results_df['p_value'] > 0.05\n",
    "\n",
    "    return simulations_results_dict, subset_statistics_df, normality_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cb9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for mdr_sets_out_sample_direct_t126, moving on\n",
      "Done for mdr_sets_out_sample_direct_t189, moving on\n",
      "Done for mdr_sets_out_sample_direct_t252, moving on\n",
      "Done for equalw_sets_out_sample_direct_t126, moving on\n",
      "Done for equalw_sets_out_sample_direct_t189, moving on\n",
      "Done for equalw_sets_out_sample_direct_t252, moving on\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_overall = 'NEW_RESULTS_FINAL/'\n",
    "n_sims = 1000\n",
    "data_returns = returns_all\n",
    "data_name = 'simulation'\n",
    "distribution_models = ['bootstrap', 'multivar_t']\n",
    "time_periods = [126, 189, 252]\n",
    "portfolio_sets_group = {'mdr_sets': mdr_reoptimized_sets, 'equalw_sets': equalw_sets}\n",
    "windsorize = False\n",
    "folder = 'simulation_results_FINAL_new_bounds'\n",
    "\n",
    "\n",
    "MODE = 'OUT_OF_SAMPLE_DIRECT' \n",
    "\n",
    "\n",
    "if MODE == 'OUT_OF_SAMPLE_DIRECT':\n",
    "    data_returns = returns_all_24\n",
    "    data_name = 'out_sample_direct'\n",
    "    distribution_models = ['out_sample_direct']\n",
    "    windsorize = False\n",
    "    folder = 'out_of_sample_direct_results_FINAL_new_bounds'\n",
    "\n",
    "folder = folder_overall + folder\n",
    "# Make sure output folders exist\n",
    "os.makedirs(f'{folder}', exist_ok=True)\n",
    "\n",
    "for set_name, set_itself in portfolio_sets_group.items():\n",
    "    for distribution_model in distribution_models:\n",
    "        for time_period in time_periods:\n",
    "            subset_statistics_results_dfs = dict()\n",
    "            normality_results_dfs = dict()\n",
    "            results_all_df = pd.DataFrame()\n",
    "\n",
    "            for key, portfolio_set in set_itself.items():\n",
    "                simulations_results_dict, subset_statistics_df, normality = simulate_evaluate_portfolio_subset(\n",
    "                    portfolio_set,\n",
    "                    data_returns,\n",
    "                    n_sims=n_sims,\n",
    "                    t=time_period,\n",
    "                    distribution_model=distribution_model,\n",
    "                    winsorize=windsorize,\n",
    "                    winsorize_limits=(0.01, 0.01),\n",
    "                    use_log_returns=True\n",
    "                )\n",
    "\n",
    "                # Initialize nested dicts\n",
    "                subset_statistics_results_dfs.setdefault(time_period, {})\n",
    "                normality_results_dfs.setdefault(time_period, {})\n",
    "\n",
    "                subset_statistics_results_dfs[time_period][key] = subset_statistics_df\n",
    "                normality_results_dfs[time_period][key] = normality\n",
    "\n",
    "                # Mean series and concat\n",
    "                mean_series = subset_statistics_df.mean()\n",
    "                mean_df = pd.DataFrame(mean_series, columns=[(time_period, key)])\n",
    "                mean_df.columns = pd.MultiIndex.from_tuples(mean_df.columns)\n",
    "                results_all_df = pd.concat([results_all_df, mean_df], axis=1)\n",
    "\n",
    "            # Save to CSV with clear naming\n",
    "            csv_filename = f\"{folder}/stats_{set_name}_{distribution_model}_t{time_period}.csv\"\n",
    "            results_all_df.to_csv(csv_filename)\n",
    "\n",
    "            # Run and save Dunn-Bonferroni tests\n",
    "            all_dunn_results = dict()\n",
    "            dunn_bonferroni_test_results = dunn_bonferroni(subset_statistics_results_dfs[time_period], metrics='all')\n",
    "            all_dunn_results[time_period] = dunn_bonferroni_test_results\n",
    "\n",
    "            excel_filename = f\"{folder}/dunn_matrix_{set_name}_{distribution_model}_t{time_period}.xlsx\"\n",
    "            with pd.ExcelWriter(excel_filename) as writer:\n",
    "                for sheet_name, df in all_dunn_results[time_period].items():\n",
    "                    df.to_excel(writer, sheet_name=sheet_name[:31])\n",
    "\n",
    "\n",
    "            print(f'Done for {set_name}_{distribution_model}_t{time_period}, moving on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3a9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e025f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
