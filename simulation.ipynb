{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d97807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functions import join_stocks_crypto, generate_rand_portfolios\n",
    "from functions_post_clustering import reoptimize_weights, dunn_bonferroni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c3f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\AppData\\Local\\Temp\\ipykernel_14408\\3789361913.py:18: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  returns_all_24 = joined_df_24.pct_change().dropna()\n"
     ]
    }
   ],
   "source": [
    "#GET THE DATA IN\n",
    "\n",
    "#Main part data 2022-2023\n",
    "df_all_stocks = pd.read_csv('stocks_data_FINAL.csv',index_col='Date')\n",
    "df_all_stocks.index = pd.to_datetime(df_all_stocks.index)\n",
    "df_all_stocks.index = df_all_stocks.index.strftime('%Y-%m-%d')\n",
    "\n",
    "cryptos_df = pd.read_csv('cryptos_data_new.csv', index_col='timestamp')\n",
    "joined_df = join_stocks_crypto(cryptos_df, df_all_stocks, mode = 'stocks_left')\n",
    "joined_df.index = pd.to_datetime(joined_df.index)\n",
    "returns_all = joined_df.pct_change().dropna()\n",
    "\n",
    "#Out-of-sample data 2024:\n",
    "df_stocks_24 = pd.read_csv('stocks_data_out_sample_2024_foltered_volatility.csv',index_col='Date')\n",
    "cryptos_df_24 = pd.read_csv('cryptos_data_out_sample_2024.csv', index_col='timestamp')\n",
    "joined_df_24 = join_stocks_crypto(cryptos_df_24, df_stocks_24, mode = 'stocks_left')\n",
    "joined_df_24.index = pd.to_datetime(joined_df_24.index)\n",
    "returns_all_24 = joined_df_24.pct_change().dropna()\n",
    "\n",
    "\n",
    "tickers = list(df_all_stocks.columns)\n",
    "\n",
    "random.seed(42)\n",
    "random_portfolios = generate_rand_portfolios(n_reps=1000, n_stocks=15, tickers=tickers)\n",
    "\n",
    "\n",
    "# #Reassemble the portfolio jsons for minvar\n",
    "# min_var_portfolios = dict()\n",
    "# for i in range(1,1000):\n",
    "#     with open(f'min_variance_portfolio_jsons/my_dict{i}.json') as f:\n",
    "#         port = json.load(f)\n",
    "#         min_var_portfolios.update(port)\n",
    "\n",
    "\n",
    "\n",
    "# with open('minvar_reoptimized_sets.json') as f:\n",
    "#     minvar_reoptimized_sets = json.load(f)\n",
    "\n",
    "with open('equalw_sets.json') as f:\n",
    "    equalw_sets = json.load(f)\n",
    "\n",
    "with open('mdr_reoptimized_sets.json') as f:\n",
    "    mdr_reoptimized_sets = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e0599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_crypto_overlap(portfolio1, portfolio2, crypto_assets):\n",
    "#     \"\"\"\n",
    "#     Compare crypto overlap between two portfolios.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     portfolio1, portfolio2 : list or dict\n",
    "#         Portfolio assets (if dict, uses keys)\n",
    "#     crypto_assets : list\n",
    "#         List of crypto tickers to filter for\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict\n",
    "#         Overlap statistics and details\n",
    "#     \"\"\"\n",
    "#     # Extract assets from portfolios\n",
    "#     if isinstance(portfolio1, dict):\n",
    "#         assets1 = list(portfolio1.keys())\n",
    "#     else:\n",
    "#         print(\"AAAA\")\n",
    "#     #     assets1 = portfolio1\n",
    "        \n",
    "#     if isinstance(portfolio2, dict):\n",
    "#         assets2 = list(portfolio2.keys())\n",
    "#     else:\n",
    "#         assets2 = portfolio2\n",
    "    \n",
    "#     # Filter for cryptos only\n",
    "#     cryptos1 = set([asset for asset in assets1 if asset in crypto_assets])\n",
    "#     cryptos2 = set([asset for asset in assets2 if asset in crypto_assets])\n",
    "    \n",
    "#     # Calculate overlap\n",
    "#     intersection = cryptos1 & cryptos2\n",
    "#     union = cryptos1 | cryptos2\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     jaccard = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "#     overlap_pct = len(intersection) / min(len(cryptos1), len(cryptos2)) if min(len(cryptos1), len(cryptos2)) > 0 else 0\n",
    "    \n",
    "#     output =  {\n",
    "#         'cryptos_portfolio1': sorted(list(cryptos1)),\n",
    "#         'cryptos_portfolio2': sorted(list(cryptos2)),\n",
    "#         'common_cryptos': sorted(list(intersection)),\n",
    "#         'unique_to_p1': sorted(list(cryptos1 - cryptos2)),\n",
    "#         'unique_to_p2': sorted(list(cryptos2 - cryptos1)),\n",
    "#         'overlap_count': len(intersection),\n",
    "#         'jaccard_similarity': jaccard,\n",
    "#         'overlap_percentage': overlap_pct\n",
    "#     }\n",
    "\n",
    "#     if len(sorted(list(intersection))) < 3:\n",
    "#         print('YES')\n",
    "#     return \n",
    "\n",
    "# CRYPTO_ASSETS = list(cryptos_df.columns)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     kmeans_port = crypto_supplemented_sets['kmeans_crypto'][f'portfolio_{i}']\n",
    "#     kshape_port = crypto_supplemented_sets['kshape_crypto'][f'portfolio_{i}']\n",
    "\n",
    "#     output = compare_crypto_overlap(kmeans_port, kshape_port, CRYPTO_ASSETS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7909549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130b740f",
   "metadata": {},
   "source": [
    "SIMULATE AND EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66ea3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL CLAUDE GENERATION CODE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import mstats, normaltest\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import random\n",
    "\n",
    "# Missing function: calculate_cumulative_returns\n",
    "def calculate_cumulative_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate cumulative returns from daily returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : numpy.ndarray\n",
    "        Array of daily returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Cumulative returns\n",
    "    \"\"\"\n",
    "    return np.cumprod(1 + returns, axis=0) - 1\n",
    "\n",
    "# Missing function: calculate_var_cvar\n",
    "def calculate_var_cvar(returns, confidence_level=0.05, initial_value=100):\n",
    "    \"\"\"\n",
    "    Calculate Value at Risk (VaR) and Conditional Value at Risk (CVaR)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : numpy.ndarray\n",
    "        Array of daily returns\n",
    "    confidence_level : float\n",
    "        Confidence level for VaR calculation (default: 0.05)\n",
    "    initial_value : float\n",
    "        Initial portfolio value (default: 100)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (VaR, CVaR)\n",
    "    \"\"\"\n",
    "    VaR = abs(np.percentile(returns, confidence_level * 100)) * initial_value\n",
    "    worst_losses = returns[returns <= -VaR/initial_value]\n",
    "    CVaR = abs(worst_losses.mean()) * initial_value if len(worst_losses) > 0 else VaR\n",
    "    \n",
    "    return VaR, CVaR\n",
    "\n",
    "def run_simulation(portfolio_dict, returns_for_portfolio, n_sims=100, t=100, \n",
    "                  distribution_model='multivar_norm', plot=False, initialPortfolio=100, \n",
    "                  winsorize=False, winsorize_limits=(0.01, 0.01), use_log_returns=False):\n",
    "    \"\"\"\n",
    "    Run Monte Carlo simulation for portfolio returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolio_dict : dict\n",
    "        Dictionary with tickers as keys and weights as values\n",
    "    returns_for_portfolio : pandas.DataFrame\n",
    "        DataFrame with dates as index and tickers as columns containing returns\n",
    "    n_sims : int\n",
    "        Number of simulations to run\n",
    "    t : int\n",
    "        Number of time periods to simulate\n",
    "    distribution_model : str\n",
    "        Distribution model to use ('multivar_norm', 'multivar_t', 'bootstrap')\n",
    "    plot : bool\n",
    "        Whether to plot the simulation results\n",
    "    initialPortfolio : float\n",
    "        Initial portfolio value\n",
    "    winsorize : bool\n",
    "        Whether to winsorize returns\n",
    "    winsorize_limits : tuple\n",
    "        Limits for winsorization\n",
    "    use_log_returns : bool\n",
    "        Whether to use log returns (for consistency with out-of-sample analysis)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Simulated portfolio paths\n",
    "    \"\"\"\n",
    "    returns_for_portfolio = returns_for_portfolio[list(portfolio_dict.keys())]\n",
    "    \n",
    "    if winsorize:\n",
    "        winsorized_returns = returns_for_portfolio.copy()\n",
    "        \n",
    "        for col in winsorized_returns.columns:\n",
    "            winsorized_returns[col] = mstats.winsorize(winsorized_returns[col], limits=winsorize_limits)\n",
    "        \n",
    "        returns_for_portfolio = winsorized_returns\n",
    "\n",
    "    # Convert to log returns if specified (for consistency with out-of-sample)\n",
    "    if use_log_returns:\n",
    "        invalid_mask = returns_for_portfolio <= -1.0\n",
    "        if invalid_mask.any().any():\n",
    "            log_compatible_returns = returns_for_portfolio.copy()\n",
    "            log_compatible_returns[invalid_mask] = -0.99\n",
    "            log_returns = np.log(1 + log_compatible_returns)\n",
    "            returns_for_portfolio = log_returns\n",
    "        else:\n",
    "            returns_for_portfolio = np.log(1 + returns_for_portfolio)\n",
    "\n",
    "    mean_returns = returns_for_portfolio.mean()\n",
    "    cov_matrix = returns_for_portfolio.cov()\n",
    "\n",
    "    weights = np.array([v for _, v in portfolio_dict.items()])\n",
    "\n",
    "    meanM = np.tile(mean_returns, (t, 1))  # Shape: (T, n_assets)\n",
    "\n",
    "    portfolio_sims = np.zeros((t, n_sims))\n",
    "\n",
    "    L = np.linalg.cholesky(cov_matrix)  # Cholesky decomposition\n",
    "\n",
    "    for sim in range(n_sims):\n",
    "        if distribution_model == 'bootstrap':\n",
    "            sampled_returns = returns_for_portfolio.bfill().sample(n=t, replace=True).values\n",
    "            portfolio_returns = sampled_returns @ weights\n",
    "        elif distribution_model in ['multivar_norm', 'multivar_t']:\n",
    "            if distribution_model == 'multivar_norm':\n",
    "                Z = np.random.normal(size=(t, len(portfolio_dict)))  # Shape: (T, n_assets)\n",
    "                daily_returns = meanM + Z @ L.T  # Shape: (T, n_assets)\n",
    "            elif distribution_model == 'multivar_t':\n",
    "                df = 25  # degrees of freedom\n",
    "                Z = np.random.normal(size=(t, len(portfolio_dict)))\n",
    "                chi2 = np.random.chisquare(df, size=(t, 1))\n",
    "                Z_t = Z / np.sqrt(chi2 / df)  # now Z_t has t-distributed marginals\n",
    "\n",
    "                daily_returns = meanM + Z_t @ L.T\n",
    "\n",
    "            portfolio_returns = daily_returns @ weights  # Shape: (T,)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # Convert back from log returns if necessary\n",
    "        if use_log_returns:\n",
    "            portfolio_returns = np.exp(portfolio_returns) - 1\n",
    "        \n",
    "        portfolio_sims[:, sim] = np.cumprod(1 + portfolio_returns) * initialPortfolio\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(portfolio_sims)\n",
    "        plt.title(\"Monte Carlo Simulated Portfolio Paths\")\n",
    "        plt.xlabel(\"Days\")\n",
    "        plt.ylabel(\"Portfolio Value\")\n",
    "        plt.show()\n",
    "\n",
    "    return portfolio_sims\n",
    "\n",
    "def out_of_sample_analysis(portfolio, return_data, start_date, period=126, \n",
    "                          calculate_metrics=True, plot_results=False, risk_free_rate=0.02, \n",
    "                          use_log_returns=False):\n",
    "    \"\"\"\n",
    "    Perform traditional out-of-sample analysis for a portfolio over a specified forward period.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolio : dict\n",
    "        Dictionary with tickers as keys and weights as values\n",
    "    price_data : pandas.DataFrame\n",
    "        DataFrame with dates as index and tickers as columns containing returns\n",
    "    start_date : str or datetime\n",
    "        Date marking the beginning of out-of-sample period\n",
    "    period : int\n",
    "        Number of days for the forward period to analyze (default 126 ~= 6 months of trading days)\n",
    "    calculate_metrics : bool\n",
    "        Whether to calculate and return performance metrics\n",
    "    plot_results : bool\n",
    "        Whether to generate and return performance plots\n",
    "    risk_free_rate : float\n",
    "        Annual risk-free rate used for Sharpe ratio calculation\n",
    "    use_log_returns : bool\n",
    "        Whether to use log returns (for consistency with simulation)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing performance metrics and return series for the period\n",
    "    \"\"\"\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "    \n",
    "    results = {}\n",
    "    tickers = list(portfolio.keys())\n",
    "    weights = np.array([portfolio[ticker] for ticker in tickers])\n",
    "    \n",
    "    missing_tickers = [ticker for ticker in tickers if ticker not in return_data.columns]\n",
    "    if missing_tickers:\n",
    "        raise ValueError(f\"The following tickers are missing from price_data: {missing_tickers}\")\n",
    "    \n",
    "    return_subset = return_data[tickers].copy()\n",
    "    return_subset = return_subset.sort_index()\n",
    "    oos_data = return_subset[return_subset.index >= start_date]\n",
    "    \n",
    "    if oos_data.empty:\n",
    "        raise ValueError(f\"No data available after start_date: {start_date}\")\n",
    "    \n",
    "    period_data = oos_data.iloc[:period]\n",
    "    \n",
    "    # Calculate daily returns from prices\n",
    "    returns = period_data.copy()\n",
    "\n",
    "    if use_log_returns:\n",
    "        invalid_mask = returns <= -1.0\n",
    "        # if invalid_mask.any().any():\n",
    "        #     log_compatible_returns = returns.copy()\n",
    "        #     log_compatible_returns[invalid_mask] = -0.99\n",
    "        #     log_returns = np.log(1 + log_compatible_returns)\n",
    "        # else:\n",
    "        #     log_returns = np.log(1 + returns)\n",
    "            \n",
    "        # portfolio_log_returns = log_returns.dot(weights)\n",
    "        \n",
    "        # # Convert back to simple returns for statistics\n",
    "        # portfolio_returns = np.exp(portfolio_log_returns) - 1\n",
    "        \n",
    "        # # Calculate cumulative returns properly from log returns\n",
    "        # cumulative_returns = np.exp(portfolio_log_returns.cumsum()) - 1\n",
    "    else:\n",
    "        # Simple returns method\n",
    "        portfolio_returns = returns.dot(weights)\n",
    "        cumulative_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "\n",
    "    results = {\n",
    "        'returns': portfolio_returns,\n",
    "        'cumulative_returns': cumulative_returns\n",
    "    }\n",
    "    \n",
    "    if calculate_metrics:\n",
    "        total_return = cumulative_returns.iloc[-1] if len(cumulative_returns) > 0 else 0\n",
    "        trading_days = min(len(portfolio_returns), 252)\n",
    "        \n",
    "        # Annualization factor\n",
    "        annual_factor = 252 / trading_days\n",
    "        annualized_return = (1 + total_return) ** annual_factor - 1\n",
    "\n",
    "        volatility = portfolio_returns.std() * np.sqrt(252)\n",
    "        sharpe_ratio = (annualized_return - risk_free_rate) / volatility if volatility > 0 else 0\n",
    "        \n",
    "        # Non-annualized Sharpe ratio for the period\n",
    "        rf_period = risk_free_rate * trading_days / 252  # Risk-free rate for the period\n",
    "        volatility_period = portfolio_returns.std() * np.sqrt(trading_days)\n",
    "        sharpe_period = (total_return - rf_period) / volatility_period if volatility_period > 0 else 0\n",
    "        \n",
    "        # Calculate VaR\n",
    "        var_95 = abs(portfolio_returns.quantile(0.05)) * 100  # Scale to match the simulation's calculation\n",
    "        \n",
    "        results.update({\n",
    "            'mean_period_return': total_return,\n",
    "            'annualised_return': annualized_return,\n",
    "            'sharpe_annualized': sharpe_ratio,\n",
    "            'sharpe_period': sharpe_period,\n",
    "            'var_95': var_95,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_diversification_ratio(portfolio_dict, return_df):\n",
    "    \"\"\"\n",
    "    Calculate the diversification ratio for a portfolio.\n",
    "    \n",
    "    Diversification Ratio = (Weighted Average of Individual Asset Volatilities) / (Portfolio Volatility)\n",
    "    \n",
    "    A ratio > 1 indicates diversification benefits (portfolio volatility < weighted average of individual volatilities)\n",
    "    A ratio = 1 indicates no diversification benefits (perfect correlation)\n",
    "    A ratio < 1 is theoretically impossible for long-only portfolios\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolio_dict : dict\n",
    "        Dictionary mapping ticker symbols to weights\n",
    "    return_df : pandas.DataFrame\n",
    "        DataFrame with returns data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Diversification ratio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the assets in the portfolio\n",
    "        assets = list(portfolio_dict.keys())\n",
    "        weights = np.array([portfolio_dict[asset] for asset in assets])\n",
    "        \n",
    "        # Filter return data to only include portfolio assets\n",
    "        portfolio_returns = return_df[assets].dropna()\n",
    "        \n",
    "        if len(portfolio_returns) < 30:  # Need minimum data for reliable volatility estimates\n",
    "            return np.nan\n",
    "        \n",
    "        # Calculate individual asset volatilities (annualized)\n",
    "        individual_volatilities = portfolio_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Calculate weighted average of individual volatilities\n",
    "        weighted_avg_volatility = np.sum(weights * individual_volatilities)\n",
    "        \n",
    "        # Calculate portfolio returns\n",
    "        portfolio_return_series = (portfolio_returns * weights).sum(axis=1)\n",
    "        \n",
    "        # Calculate portfolio volatility (annualized)\n",
    "        portfolio_volatility = portfolio_return_series.std() * np.sqrt(252)\n",
    "        \n",
    "        # Calculate diversification ratio\n",
    "        if portfolio_volatility > 0:\n",
    "            diversification_ratio = weighted_avg_volatility / portfolio_volatility\n",
    "            return diversification_ratio\n",
    "        else:\n",
    "            return np.nan\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating diversification ratio: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def simulate_evaluate_portfolio_subset(portfolios_subset, return_df, n_sims=100, t=100, \n",
    "                                      distribution_model='multivar_norm', rf_annual=0.04, \n",
    "                                      seed=30, winsorize=False, winsorize_limits=(0.01, 0.01),\n",
    "                                      use_log_returns=False):\n",
    "    \"\"\"\n",
    "    Simulate and evaluate a subset of portfolios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolios_subset : dict\n",
    "        Dictionary of dictionaries, where the outer keys are portfolio identifiers\n",
    "        and the inner dictionaries are ticker->weight mappings\n",
    "    return_df : pandas.DataFrame\n",
    "        DataFrame with dates as index and tickers as columns containing RETURNS\n",
    "    n_sims : int\n",
    "        Number of simulations to run for each portfolio\n",
    "    t : int\n",
    "        Number of time periods to simulate\n",
    "    distribution_model : str\n",
    "        Distribution model to use ('multivar_norm', 'multivar_t', 'bootstrap', 'out_sample_direct')\n",
    "    rf_annual : float\n",
    "        Annual risk-free rate\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    winsorize : bool\n",
    "        Whether to winsorize returns\n",
    "    winsorize_limits : tuple\n",
    "        Limits for winsorization\n",
    "    use_log_returns : bool\n",
    "        Whether to use log returns for consistency between methods\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (simulation_results, statistics, normality_test_results)\n",
    "    \"\"\"\n",
    "    simulations_results_dict = dict()\n",
    "    subset_statistics_df = pd.DataFrame()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for i, portfolio_dict in portfolios_subset.items():\n",
    "        # Making sure weights sum up to 1\n",
    "        total = sum(portfolio_dict.values())\n",
    "        portfolio_dict = {k: v / total for k, v in portfolio_dict.items()}\n",
    "        \n",
    "        # Calculate diversification ratio for this portfolio\n",
    "        diversification_ratio = calculate_diversification_ratio(portfolio_dict, return_df)\n",
    "\n",
    "        if distribution_model == 'out_sample_direct':\n",
    "        \n",
    "            res = out_of_sample_analysis(\n",
    "                portfolio_dict, \n",
    "                return_df, \n",
    "                '2024-01-01', \n",
    "                period=t, \n",
    "                calculate_metrics=True, \n",
    "                risk_free_rate=rf_annual, \n",
    "                plot_results=False, \n",
    "                use_log_returns=use_log_returns\n",
    "            )\n",
    "            \n",
    "            mean_annual_return = res['annualised_return']\n",
    "            mean_return = res['mean_period_return']\n",
    "            sharpe_annual = res['sharpe_annualized']\n",
    "            VaR_final = res['var_95']\n",
    "            sharpe_period = res['sharpe_period']\n",
    "\n",
    "            stat_results = pd.DataFrame({\n",
    "                'annualised_return': [mean_annual_return],\n",
    "                'mean_period_return': [mean_return],\n",
    "                'sharpe_annualized': [sharpe_annual],\n",
    "                'sharpe_period': [sharpe_period],\n",
    "                'VaR': [VaR_final],\n",
    "                'diversification_ratio': [diversification_ratio],\n",
    "            })\n",
    "            \n",
    "            subset_statistics_df = pd.concat([subset_statistics_df, stat_results])\n",
    "            \n",
    "        else:\n",
    "            # Run simulation with consistent parameters\n",
    "            portfolio_sims = run_simulation(\n",
    "                portfolio_dict, \n",
    "                return_df[list(portfolio_dict.keys())], \n",
    "                n_sims=n_sims, \n",
    "                t=t, \n",
    "                distribution_model=distribution_model, \n",
    "                plot=False, \n",
    "                winsorize=winsorize, \n",
    "                winsorize_limits=winsorize_limits,\n",
    "                use_log_returns=use_log_returns\n",
    "            )\n",
    "\n",
    "            simulations_results_dict[i] = portfolio_sims\n",
    "            \n",
    "            daily_returns = (portfolio_sims[1:, :] - portfolio_sims[:-1, :]) / portfolio_sims[:-1, :]\n",
    "            \n",
    "            # For each simulation path, calculate key metrics\n",
    "            all_stats = []\n",
    "            \n",
    "            for sim in range(n_sims):\n",
    "                # Get the returns for this simulation\n",
    "                sim_returns = daily_returns[:, sim]\n",
    "                \n",
    "                # Calculate cumulative return for the period\n",
    "                final_value = portfolio_sims[-1, sim]\n",
    "                initial_value = 100\n",
    "                total_return = (final_value - initial_value) / initial_value\n",
    "                \n",
    "                # Calculate trading days (consistent with out-of-sample)\n",
    "                trading_days = min(len(sim_returns), 252)\n",
    "                \n",
    "                # Annualization factor\n",
    "                annual_factor = 252 / trading_days\n",
    "                annualized_return = (1 + total_return) ** annual_factor - 1\n",
    "                \n",
    "                # Calculate volatility (annualized)\n",
    "                volatility = sim_returns.std() * np.sqrt(252)\n",
    "                \n",
    "                # Calculate Sharpe ratio (annualized)\n",
    "                sharpe_annual = (annualized_return - rf_annual) / volatility if volatility > 0 else 0\n",
    "                \n",
    "                # Calculate period Sharpe ratio\n",
    "                rf_period = rf_annual * trading_days / 252\n",
    "                volatility_period = sim_returns.std() * np.sqrt(trading_days)\n",
    "                sharpe_period = (total_return - rf_period) / volatility_period if volatility_period > 0 else 0\n",
    "                \n",
    "                # Calculate VaR\n",
    "                var_95 = abs(np.percentile(sim_returns, 5)) * 100\n",
    "                \n",
    "                all_stats.append({\n",
    "                    'annualised_return': annualized_return,\n",
    "                    'mean_period_return': total_return,\n",
    "                    'sharpe_annualized': sharpe_annual,\n",
    "                    'sharpe_period': sharpe_period,\n",
    "                    'VaR': var_95\n",
    "                })\n",
    "            \n",
    "            # Convert all simulations to a dataframe\n",
    "            sim_stats_df = pd.DataFrame(all_stats)\n",
    "            \n",
    "            # Calculate averages across all simulations\n",
    "            mean_annual_return = sim_stats_df['annualised_return'].mean()\n",
    "            mean_return = sim_stats_df['mean_period_return'].mean()\n",
    "            mean_sharpe_annual = sim_stats_df['sharpe_annualized'].mean()\n",
    "            mean_sharpe_period = sim_stats_df['sharpe_period'].mean()\n",
    "            mean_var = sim_stats_df['VaR'].mean()\n",
    "            \n",
    "            stat_results = pd.DataFrame({\n",
    "                'annualised_return': [mean_annual_return],\n",
    "                'mean_period_return': [mean_return],\n",
    "                'sharpe_annualized': [mean_sharpe_annual],\n",
    "                'sharpe_period': [mean_sharpe_period],\n",
    "                'VaR': [mean_var],\n",
    "                'diversification_ratio': [diversification_ratio],\n",
    "            })\n",
    "\n",
    "            subset_statistics_df = pd.concat([subset_statistics_df, stat_results])\n",
    "\n",
    "    subset_statistics_df = subset_statistics_df.reset_index(drop=True)\n",
    "\n",
    "    # Run normality test (including diversification ratio)\n",
    "    results_normality_test = {}\n",
    "    for col in subset_statistics_df.columns:\n",
    "        stat, p_value = normaltest(subset_statistics_df[col])\n",
    "        results_normality_test[col] = {'statistic': stat, 'p_value': p_value}\n",
    "\n",
    "    normality_results_df = pd.DataFrame(results_normality_test).T\n",
    "    normality_results_df['normal'] = normality_results_df['p_value'] > 0.05\n",
    "\n",
    "    return simulations_results_dict, subset_statistics_df, normality_results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb9ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m results_all_df = pd.DataFrame()\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, portfolio_set \u001b[38;5;129;01min\u001b[39;00m set_itself.items():\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     simulations_results_dict, subset_statistics_df, normality = \u001b[43msimulate_evaluate_portfolio_subset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mportfolio_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_sims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_sims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistribution_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistribution_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwinsorize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindsorize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwinsorize_limits\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Initialize nested dicts\u001b[39;00m\n\u001b[32m     52\u001b[39m     subset_statistics_results_dfs.setdefault(time_period, {})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 367\u001b[39m, in \u001b[36msimulate_evaluate_portfolio_subset\u001b[39m\u001b[34m(portfolios_subset, return_df, n_sims, t, distribution_model, rf_annual, seed, winsorize, winsorize_limits, use_log_returns)\u001b[39m\n\u001b[32m    364\u001b[39m portfolio_dict = {k: v / total \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m portfolio_dict.items()}\n\u001b[32m    366\u001b[39m \u001b[38;5;66;03m# Calculate diversification ratio for this portfolio\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m diversification_ratio = \u001b[43mcalculate_diversification_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mportfolio_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m distribution_model == \u001b[33m'\u001b[39m\u001b[33mout_sample_direct\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    371\u001b[39m     res = out_of_sample_analysis(\n\u001b[32m    372\u001b[39m         portfolio_dict, \n\u001b[32m    373\u001b[39m         return_df, \n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m         use_log_returns=use_log_returns\n\u001b[32m    380\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 290\u001b[39m, in \u001b[36mcalculate_diversification_ratio\u001b[39m\u001b[34m(portfolio_dict, return_df)\u001b[39m\n\u001b[32m    287\u001b[39m weights = np.array([portfolio_dict[asset] \u001b[38;5;28;01mfor\u001b[39;00m asset \u001b[38;5;129;01min\u001b[39;00m assets])\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Filter return data to only include portfolio assets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m portfolio_returns = \u001b[43mreturn_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43massets\u001b[49m\u001b[43m]\u001b[49m.dropna()\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(portfolio_returns) < \u001b[32m30\u001b[39m:  \u001b[38;5;66;03m# Need minimum data for reliable volatility estimates\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.nan\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\frame.py:4117\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   4115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._slice(indexer, axis=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4117\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4120\u001b[39m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[32m   4121\u001b[39m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[32m   4122\u001b[39m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[32m   4123\u001b[39m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[32m   4124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n\u001b[32m   4125\u001b[39m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4142\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4145\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4146\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4151\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4153\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4154\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4128\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4129\u001b[39m     indices = np.arange(\n\u001b[32m   4130\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4131\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4133\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4139\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4140\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m    688\u001b[39m         blk.take_nd(\n\u001b[32m    689\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    841\u001b[39m                     blocks.append(nb)\n\u001b[32m    842\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m                 blocks.append(nb)\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1304\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1315\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kirill\\Documents\\Projects\\thesis\\venv312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_overall = 'NEW_RESULTS_FINAL/'\n",
    "n_sims = 100\n",
    "data_returns = returns_all\n",
    "data_name = 'simulation'\n",
    "distribution_models = ['bootstrap', 'multivar_t', 'multivar_norm']\n",
    "time_periods = [126, 189, 252]\n",
    "portfolio_sets_group = {'equalw_sets': equalw_sets} #{'mdr_sets': mdr_reoptimized_sets}, #'\n",
    "windsorize = True\n",
    "folder = 'simulation_results'\n",
    "\n",
    "\n",
    "MODE = 'SIMULATION' \n",
    "\n",
    "if MODE == 'OUT_OF_SAMPLE_BOOTSTRAP':\n",
    "    data_returns = returns_all_24\n",
    "    data_name = 'simulation'\n",
    "    distribution_models = ['bootstrap']\n",
    "    windsorize = False\n",
    "    folder = 'out_of_sample_bootstrap_results'\n",
    "\n",
    "if MODE == 'OUT_OF_SAMPLE_DIRECT':\n",
    "    data_returns = returns_all_24\n",
    "    data_name = 'out_sample_direct'\n",
    "    distribution_models = ['out_sample_direct']\n",
    "    windsorize = False\n",
    "    folder = 'out_of_sample_direct_results'\n",
    "\n",
    "folder = folder_overall + folder\n",
    "# Make sure output folders exist\n",
    "os.makedirs(f'{folder}', exist_ok=True)\n",
    "\n",
    "for set_name, set_itself in portfolio_sets_group.items():\n",
    "    for distribution_model in distribution_models:\n",
    "        for time_period in time_periods:\n",
    "            subset_statistics_results_dfs = dict()\n",
    "            normality_results_dfs = dict()\n",
    "            results_all_df = pd.DataFrame()\n",
    "\n",
    "            for key, portfolio_set in set_itself.items():\n",
    "                simulations_results_dict, subset_statistics_df, normality = simulate_evaluate_portfolio_subset(\n",
    "                    portfolio_set,\n",
    "                    data_returns,\n",
    "                    n_sims=n_sims,\n",
    "                    t=time_period,\n",
    "                    distribution_model=distribution_model,\n",
    "                    winsorize=windsorize,\n",
    "                    winsorize_limits=(0.01, 0.01)\n",
    "                )\n",
    "\n",
    "                # Initialize nested dicts\n",
    "                subset_statistics_results_dfs.setdefault(time_period, {})\n",
    "                normality_results_dfs.setdefault(time_period, {})\n",
    "\n",
    "                subset_statistics_results_dfs[time_period][key] = subset_statistics_df\n",
    "                normality_results_dfs[time_period][key] = normality\n",
    "\n",
    "                # Mean series and concat\n",
    "                mean_series = subset_statistics_df.mean()\n",
    "                mean_df = pd.DataFrame(mean_series, columns=[(time_period, key)])\n",
    "                mean_df.columns = pd.MultiIndex.from_tuples(mean_df.columns)\n",
    "                results_all_df = pd.concat([results_all_df, mean_df], axis=1)\n",
    "\n",
    "            # Save to CSV with clear naming\n",
    "            csv_filename = f\"{folder}/stats_{set_name}_{distribution_model}_t{time_period}.csv\"\n",
    "            results_all_df.to_csv(csv_filename)\n",
    "\n",
    "            # Run and save Dunn-Bonferroni tests\n",
    "            all_dunn_results = dict()\n",
    "            dunn_bonferroni_test_results = dunn_bonferroni(subset_statistics_results_dfs[time_period], metrics='all')\n",
    "            all_dunn_results[time_period] = dunn_bonferroni_test_results\n",
    "\n",
    "            # Save to Excel\n",
    "            excel_filename = f\"{folder}/dunn_matrix_{set_name}_{distribution_model}_t{time_period}.xlsx\"\n",
    "            with pd.ExcelWriter(excel_filename) as writer:\n",
    "                for sheet_name, df in all_dunn_results[time_period].items():\n",
    "                    df.to_excel(writer, sheet_name=sheet_name[:31])\n",
    "\n",
    "\n",
    "            print(f'Done for {set_name}_{distribution_model}_t{time_period}, moving on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f3a9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24fd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d55d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
